<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-04-30T02:17:07-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Binh Vu</title><entry><title type="html">From Sketches to HTML/CSS code</title><link href="http://localhost:4000/projects/csci-599/2019/04/27/from-sketches-to-html-css-code.html" rel="alternate" type="text/html" title="From Sketches to HTML/CSS code" /><published>2019-04-27T13:08:19-07:00</published><updated>2019-04-27T13:08:19-07:00</updated><id>http://localhost:4000/projects/csci-599/2019/04/27/from-sketches-to-html-css-code</id><content type="html" xml:base="http://localhost:4000/projects/csci-599/2019/04/27/from-sketches-to-html-css-code.html">&lt;figure&gt;
	&lt;img src=&quot;/assets/20190427-sketch2code/demo.gif&quot; /&gt;
	&lt;figcaption&gt;Given the mock-up UI on the left, our system synthesizes HTML/CSS code (middle) that renders the website on the right&lt;/figcaption&gt;
&lt;/figure&gt;
# Introduction

Creating websites is a difficult task that requires expertise and a significant amount of time. In a typical web development workflow, web developers implement HTML/CSS and Javascript code based on a mock-up UI, which are created using applications such as Sketch. A task of synthesising HTML/CSS programs from mock-up UIs helps speed up the development process by allowing developers to focus more on implementing Javascript logic. 

In this post, we discuss our approaches to address the above problem, which is formally described as follow. Given a set of HTML tags $$\mathcal{T}$$, classes $$\mathcal{C}$$ of CSS libraries, and a screenshot $$I$$ of a target mock-up UI, we generate a HTML program $$P$$ that renders $$I$$. 

We experiment with two different approaches: supervised and reinforcement learning (RL). In the reinforcement learning approach, we aim to learn a Deep Q-network to synthesis HTML program without labelled data. Because the problem space in RL is enormous and we have limited resources, we have not successfully made RL to work. The details of our RL approach is described in the Appendix. In the supervised approach, we use CNN to encode the target image $$I$$ and LSTM to decode the HTML program $$P$$ from $$I$$. In our empirical evaluation, it outperforms the current state-of-the-art (pix2code) significantly by 19.7% (accuracy).

The most relevant work to this problem is pix2code, which also synthesizes HTML programs from images. Besides the different between neural network architecture, their approach first generates domain specific language (DSL) programs. Then, translate DSL programs to final HTML programs. Using DSL makes this problem easier. However, creating DSL may take lots of time and we may need to create different DSL or different DSL-to-HTML converter for different CSS libraries. By directly generating HTML/CSS program, our approach is easier to adapt to different CSS libraries, and is able to access to a tremendous amount of publicly available training data in open source projects and websites.

In the remained post, we will described different supervised models and their performances comparing to the current state-of-the-art

# Generating HTML/CSS Code with Neural-Guided Search

Similar to XML, a HTML program consists of a sequence of HTML tags, each tag can be an open tag or an close tag (e.g., `&lt;h5&gt;` and `&lt;/h5&gt;`). In addition, open tags may contain other special attributes such as `class` to indicate classes they belong to (e.g., `&lt;div class=&quot;row&quot;&gt;`). In our system, we represent a HTML program as a sequence of tokens, each token is either an open tag, an close tag, an open tag and its classes, or one of three special tokens: `#text`, `&lt;program&gt;`, `&lt;/program&gt;`. The `#text` token acts as a placeholder for text elements in the mock-up UI. `&lt;program&gt;` and `&lt;/program&gt;` indicate the begining and ending of the HTML program, repsectively.

Let $$f$$ is a function that predict probabilities of the next tokens $$x_{t+1}$$ of the program given a current sequence of tokens $$X_t$$ and a target image $$I$$. Then, the desired program $$P$$ can be generated by repeated applying $$f$$ to generate one token by one token, and optionally invoke the rendering program to evaluate the current program until we find the `&lt;/program&gt;` token.

To estimate $$f$$, we train a deep learning model that uses CNN to learn a representation vector $$z$$ of a target image, then inputs $$z$$ with current tokens $$X_t$$ of the program to an LSTM to predict the next token $$x_{t+1}$$. We experiment with three different architectures, each have different ways of usages of $$z$$ and $$X_t$$, as follow:

* **Using $$z$$ as the initial hidden state of LSTM**: the architecture of this model (ED-1) is showed in the figure below. It contains of three CNN layers extracting features from a image, then passes to a fully connected layer to extract a representation vector $$z$$ of size 512. $$z$$ is used as the hidden state of a one-layer LSTM (hidden states are vectors of $$R^{512}$$). LSTM is trained to predict next tokens of target HTML programs.

&lt;figure&gt;
	&lt;img src=&quot;/assets/20190427-sketch2code/model-ed-1.png&quot; /&gt;
	&lt;figcaption&gt;Network architecture of model ED-1&lt;/figcaption&gt;
&lt;/figure&gt;

* **Concatenating $$z$$ with each $$x_{t' \le t} \in X_t$$**: the architecture of this model (ED-2) is similar to the above model (figure below). However, instead of using $$z$$ as initial hidden state, $$z$$ is concatenate with vectors of tokens $$X_t$$ and is inputed to the LSTM.

&lt;figure&gt;
	&lt;img src=&quot;/assets/20190427-sketch2code/model-ed-2.png&quot; /&gt;
	&lt;figcaption&gt;Network architecture of model ED-2&lt;/figcaption&gt;
&lt;/figure&gt;

* **$$z$$ is computed dynamically at each time step $$t$$ using an attention layer, and is concatenated with the input token $$x_{t'}$$ to predict next token $$x_{t'+1}$$**

&lt;figure&gt;
	&lt;img src=&quot;/assets/20190427-sketch2code/model-ed-3.png&quot; /&gt;
	&lt;figcaption&gt;Network architecture of model ED-3&lt;/figcaption&gt;
&lt;/figure&gt;

# Experimental Evaluation

**Dataset and experimental setup**

We evaluate our approach on the synthesized web dataset from pix2code [1]. The dataset contains 1750 images of web pages, in which 1250, 250, and 250 pages are used for training, validation and testing, respectively. We assess the quality of predicted programs by comparing with gold programs in term of accuracies as in pix2code [1].

$$\text{accuracy(gp, pp)} = \frac{\sum_{i=0}^{min(|\text{gp}|, |\text{pp}|)} \text{gp}[i] = \text{pp}[i]}{max(|\text{gp}|, |\text{pp}|)} $$

where $$\text{gp}$$ and $$\text{pp}$$ are the predicted program and gold program, respectively, and $$\text{gp}[i]$$ is a token at position $$i$$ of the program.

**Training supervised models**

The three models (ED-1, ED-2, ED-3) are trained using ADAM-AMSGRAD optimization method for 100 epoches with learning rate 5e-4. The training task is to minimize the cross entropy loss of predicting next tokens of a program given the current tokens and a target image. We report the average loss and the average classification accuracy for predicted tokens in figures below.

&lt;figure&gt;
	&lt;img style=&quot;width: 90%&quot; src=&quot;/assets/20190427-sketch2code/training-stats.png&quot; /&gt;
	&lt;figcaption&gt;Training losses and classification accuracy per epoch&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img style=&quot;width: 90%&quot; src=&quot;/assets/20190427-sketch2code/validation-stats.png&quot; /&gt;
	&lt;figcaption&gt;Validation losses and classification accuracy per epoch&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img style=&quot;width: 90%&quot; src=&quot;/assets/20190427-sketch2code/testing-stats.png&quot; /&gt;
	&lt;figcaption&gt;Testing losses and classification accuracy per epoch&lt;/figcaption&gt;
&lt;/figure&gt;

**Automatic synthesizing HTML programs results**

| Datasets      | pix2code* | ED-1   | ED-2  | ED-3      |
| ------------- | :--------:| :-----:| :----:| :-------: |
| pix2code      | 0.794     | 0.722  | 0.982 | **0.991** |


&lt;figure&gt;
	&lt;img style=&quot;width: 90%&quot; src=&quot;/assets/20190427-sketch2code/attention.png&quot; /&gt;
	&lt;figcaption&gt;Testing losses and classification accuracy per epoch&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img style=&quot;width: 70%&quot; src=&quot;/assets/20190427-sketch2code/layer1-activation.png&quot; /&gt;
	&lt;figcaption&gt;Testing losses and classification accuracy per epoch&lt;/figcaption&gt;
&lt;/figure&gt;


# References

1. Beltramelli, Tony. “Pix2code: Generating Code from a Graphical User Interface Screenshot.” Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems, 2018, p. 3.

# Appendix

#### Reinforcement Learning Approach

**Problem formulation**

The problem of generating HTML program can be formulate to RL as follows: the state is a pair of a target GUI image and our current program. Possible actions are creating a new HTML tag, adding new CSS class to the current tag, and closing a HTML tag. The reward of a state is calculated by how similar it is between its desired GUI image, and the current webpage, which rendered from its current program.

**Reward and training**

&lt;figure&gt;
	&lt;img style=&quot;width: 70%&quot; src=&quot;/assets/20190427-sketch2code/reward.png&quot; /&gt;
	&lt;figcaption&gt;Testing losses and classification accuracy per epoch&lt;/figcaption&gt;
&lt;/figure&gt;

**Preliminary results**

&lt;figure&gt;
	&lt;img src=&quot;/assets/20190427-sketch2code/reinforcement-learning-stats.png&quot; /&gt;
	&lt;figcaption&gt;Testing losses and classification accuracy per epoch&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;style&gt;
	.post-content {
		text-align: justify;
	}
	/* make math/tex inline for figure caption */
	figcaption &gt; span {
		display: inline-block !important;
		margin: 0 !important;
	}
	figure &gt; img {
		display: block; 
		margin-left: auto; 
		margin-right: auto;
	}
	figcaption {
		text-align: center;
		font-style: italic;
		width: 80%;
		margin-left: auto;
		margin-right: auto;
	}
&lt;/style&gt;
&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</content><author><name></name></author><summary type="html">Given the mock-up UI on the left, our system synthesizes HTML/CSS code (middle) that renders the website on the right Introduction</summary></entry></feed>